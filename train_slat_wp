[SPARSE] Backend: spconv, Attention: flash_attn
Warp 1.7.1 initialized:
   CUDA Toolkit 12.8, Driver 12.1
   Devices:
     "cpu"      : "x86_64"
     "cuda:0"   : "NVIDIA A800-SXM4-80GB" (79 GiB, sm_80, mempool enabled)
     "cuda:1"   : "NVIDIA A800-SXM4-80GB" (79 GiB, sm_80, mempool enabled)
     "cuda:2"   : "NVIDIA A800-SXM4-80GB" (79 GiB, sm_80, mempool enabled)
     "cuda:3"   : "NVIDIA A800-SXM4-80GB" (79 GiB, sm_80, mempool enabled)
   CUDA peer access:
     Supported fully (all-directional)
   Kernel cache:
     /root/.cache/warp/1.7.1


Config:
================================================================================
{
    "config": "configs/generation/slat_flow_txt_dit_B_64l8p2_fp16_wp.json",
    "output_dir": "outputs/slat_flow_txt_dit_B_64l8p2_fp16_1node_finetune",
    "load_dir": "outputs/slat_flow_txt_dit_B_64l8p2_fp16_1node_finetune",
    "ckpt": "latest",
    "data_dir": "datasets/T8",
    "auto_retry": 3,
    "tryrun": false,
    "profile": false,
    "num_nodes": 1,
    "node_rank": 0,
    "num_gpus": 4,
    "master_addr": "localhost",
    "master_port": "12345",
    "models": {
        "denoiser": {
            "name": "ElasticSLatFlowModel",
            "args": {
                "resolution": 64,
                "in_channels": 8,
                "out_channels": 8,
                "model_channels": 768,
                "cond_channels": 768,
                "num_blocks": 12,
                "num_heads": 12,
                "mlp_ratio": 4,
                "patch_size": 2,
                "num_io_res_blocks": 2,
                "io_block_channels": [
                    128
                ],
                "pe_mode": "ape",
                "qk_rms_norm": true,
                "use_fp16": true
            }
        }
    },
    "dataset": {
        "name": "TextConditionedSLat",
        "args": {
            "latent_model": "dinov2_vitl14_reg_slat_enc_swin8_B_64l8_fp16",
            "min_aesthetic_score": 0,
            "max_num_voxels": 32768,
            "normalization": {
                "mean": [
                    -2.1687545776367188,
                    -0.004347046371549368,
                    -0.13352349400520325,
                    -0.08418072760105133,
                    -0.5271206498146057,
                    0.7238689064979553,
                    -1.1414450407028198,
                    1.2039363384246826
                ],
                "std": [
                    2.377650737762451,
                    2.386378288269043,
                    2.124418020248413,
                    2.1748552322387695,
                    2.663944721221924,
                    2.371192216873169,
                    2.6217446327209473,
                    2.684523105621338
                ]
            },
            "pretrained_slat_dec": "microsoft/TRELLIS-image-large/ckpts/slat_dec_gs_swin8_B_64l8gs32_fp16"
        }
    },
    "trainer": {
        "name": "TextConditionedSparseFlowMatchingCFGTrainer",
        "args": {
            "max_steps": 1000000,
            "batch_size_per_gpu": 16,
            "batch_split": 4,
            "optimizer": {
                "name": "AdamW",
                "args": {
                    "lr": 0.0001,
                    "weight_decay": 0.0
                }
            },
            "ema_rate": [
                0.9999
            ],
            "fp16_mode": "inflat_all",
            "fp16_scale_growth": 0.001,
            "elastic": {
                "name": "LinearMemoryController",
                "args": {
                    "target_ratio": 0.75,
                    "max_mem_ratio_start": 0.5
                }
            },
            "grad_clip": {
                "name": "AdaptiveGradClipper",
                "args": {
                    "max_norm": 1.0,
                    "clip_percentile": 95
                }
            },
            "i_log": 500,
            "i_sample": 2000,
            "i_save": 2000,
            "finetune_ckpt": {
                "denoiser": "/fs-computility/ai-shen/wujianyu/TRELLIS/microsoft/TRELLIS-text-base/ckpts/slat_flow_txt_dit_B_64l8p2_fp16.safetensors"
            },
            "p_uncond": 0.1,
            "t_schedule": {
                "name": "logitNormal",
                "args": {
                    "mean": 1.0,
                    "std": 1.0
                }
            },
            "sigma_min": 1e-05,
            "text_cond_model": "openai/clip-vit-large-patch14"
        }
    }
}
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  _TORCH_CUSTOM_FWD = amp.custom_fwd(cast_inputs=torch.float16)
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:97: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:163: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:243: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:332: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:369: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:389: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:412: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  _TORCH_CUSTOM_FWD = amp.custom_fwd(cast_inputs=torch.float16)
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:97: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:163: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:243: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:332: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:369: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:389: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:412: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  _TORCH_CUSTOM_FWD = amp.custom_fwd(cast_inputs=torch.float16)
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:97: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:163: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:243: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:332: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:369: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:389: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:412: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  _TORCH_CUSTOM_FWD = amp.custom_fwd(cast_inputs=torch.float16)
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:97: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:163: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:243: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:332: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:369: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:389: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:412: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
[SPARSE] Backend: spconv, Attention: flash_attn
Warp 1.7.1 initialized:
   CUDA Toolkit 12.8, Driver 12.1
   Devices:
     "cpu"      : "x86_64"
     "cuda:0"   : "NVIDIA A800-SXM4-80GB" (79 GiB, sm_80, mempool enabled)
     "cuda:1"   : "NVIDIA A800-SXM4-80GB" (79 GiB, sm_80, mempool enabled)
     "cuda:2"   : "NVIDIA A800-SXM4-80GB" (79 GiB, sm_80, mempool enabled)
     "cuda:3"   : "NVIDIA A800-SXM4-80GB" (79 GiB, sm_80, mempool enabled)
   CUDA peer access:
     Supported fully (all-directional)
   Kernel cache:
     /root/.cache/warp/1.7.1
[SPARSE][CONV] spconv algo: auto
[ATTENTION] Using backend: flash_attn
Load from safetensors
-----------------------------------
[SPARSE] Backend: spconv, Attention: flash_attn
Warp 1.7.1 initialized:
   CUDA Toolkit 12.8, Driver 12.1
   Devices:
     "cpu"      : "x86_64"
     "cuda:0"   : "NVIDIA A800-SXM4-80GB" (79 GiB, sm_80, mempool enabled)
     "cuda:1"   : "NVIDIA A800-SXM4-80GB" (79 GiB, sm_80, mempool enabled)
     "cuda:2"   : "NVIDIA A800-SXM4-80GB" (79 GiB, sm_80, mempool enabled)
     "cuda:3"   : "NVIDIA A800-SXM4-80GB" (79 GiB, sm_80, mempool enabled)
   CUDA peer access:
     Supported fully (all-directional)
   Kernel cache:
     /root/.cache/warp/1.7.1
[SPARSE][CONV] spconv algo: auto
[ATTENTION] Using backend: flash_attn
Load from safetensors
-----------------------------------
[SPARSE] Backend: spconv, Attention: flash_attn
Warp 1.7.1 initialized:
   CUDA Toolkit 12.8, Driver 12.1
   Devices:
     "cpu"      : "x86_64"
     "cuda:0"   : "NVIDIA A800-SXM4-80GB" (79 GiB, sm_80, mempool enabled)
     "cuda:1"   : "NVIDIA A800-SXM4-80GB" (79 GiB, sm_80, mempool enabled)
     "cuda:2"   : "NVIDIA A800-SXM4-80GB" (79 GiB, sm_80, mempool enabled)
     "cuda:3"   : "NVIDIA A800-SXM4-80GB" (79 GiB, sm_80, mempool enabled)
   CUDA peer access:
     Supported fully (all-directional)
   Kernel cache:
     /root/.cache/warp/1.7.1
[SPARSE][CONV] spconv algo: auto
[ATTENTION] Using backend: flash_attn
Load from safetensors
-----------------------------------
[SPARSE] Backend: spconv, Attention: flash_attn
Warp 1.7.1 initialized:
   CUDA Toolkit 12.8, Driver 12.1
   Devices:
     "cpu"      : "x86_64"
     "cuda:0"   : "NVIDIA A800-SXM4-80GB" (79 GiB, sm_80, mempool enabled)
     "cuda:1"   : "NVIDIA A800-SXM4-80GB" (79 GiB, sm_80, mempool enabled)
     "cuda:2"   : "NVIDIA A800-SXM4-80GB" (79 GiB, sm_80, mempool enabled)
     "cuda:3"   : "NVIDIA A800-SXM4-80GB" (79 GiB, sm_80, mempool enabled)
   CUDA peer access:
     Supported fully (all-directional)
   Kernel cache:
     /root/.cache/warp/1.7.1
[SPARSE][CONV] spconv algo: auto
[ATTENTION] Using backend: flash_attn


Backbone: denoiser
Parameters:
================================================================================================================================
Name                                                                    Shape                           Type            Grad
t_embedder.mlp.0.weight                                                 torch.Size([768, 256])          torch.float32   True
t_embedder.mlp.0.bias                                                   torch.Size([768])               torch.float32   True
t_embedder.mlp.2.weight                                                 torch.Size([768, 768])          torch.float32   True
t_embedder.mlp.2.bias                                                   torch.Size([768])               torch.float32   True
input_layer.weight                                                      torch.Size([128, 8])            torch.float32   True
input_layer.bias                                                        torch.Size([128])               torch.float32   True
input_blocks.0.norm1.weight                                             torch.Size([128])               torch.float32   True
input_blocks.0.norm1.bias                                               torch.Size([128])               torch.float32   True
input_blocks.0.conv1.conv.weight                                        torch.Size([128, 3, 3, 3, 128]) torch.float16   True
input_blocks.0.conv1.conv.bias                                          torch.Size([128])               torch.float16   True
input_blocks.0.conv2.conv.weight                                        torch.Size([128, 3, 3, 3, 128]) torch.float16   True
input_blocks.0.conv2.conv.bias                                          torch.Size([128])               torch.float16   True
input_blocks.0.emb_layers.1.weight                                      torch.Size([256, 768])          torch.float16   True
input_blocks.0.emb_layers.1.bias                                        torch.Size([256])               torch.float16   True
input_blocks.1.norm1.weight                                             torch.Size([128])               torch.float32   True
input_blocks.1.norm1.bias                                               torch.Size([128])               torch.float32   True
input_blocks.1.conv1.conv.weight                                        torch.Size([768, 3, 3, 3, 128]) torch.float16   True
input_blocks.1.conv1.conv.bias                                          torch.Size([768])               torch.float16   True
input_blocks.1.conv2.conv.weight                                        torch.Size([768, 3, 3, 3, 768]) torch.float16   True
input_blocks.1.conv2.conv.bias                                          torch.Size([768])               torch.float16   True
input_blocks.1.emb_layers.1.weight                                      torch.Size([1536, 768])         torch.float16   True
input_blocks.1.emb_layers.1.bias                                        torch.Size([1536])              torch.float16   True
input_blocks.1.skip_connection.weight                                   torch.Size([768, 128])          torch.float16   True
input_blocks.1.skip_connection.bias                                     torch.Size([768])               torch.float16   True
blocks.0.norm2.weight                                                   torch.Size([768])               torch.float32   True
blocks.0.norm2.bias                                                     torch.Size([768])               torch.float32   True
blocks.0.self_attn.to_qkv.weight                                        torch.Size([2304, 768])         torch.float16   True
blocks.0.self_attn.to_qkv.bias                                          torch.Size([2304])              torch.float16   True
blocks.0.self_attn.q_rms_norm.gamma                                     torch.Size([12, 64])            torch.float32   True
blocks.0.self_attn.k_rms_norm.gamma                                     torch.Size([12, 64])            torch.float32   True
blocks.0.self_attn.to_out.weight                                        torch.Size([768, 768])          torch.float16   True
blocks.0.self_attn.to_out.bias                                          torch.Size([768])               torch.float16   True
blocks.0.cross_attn.to_q.weight                                         torch.Size([768, 768])          torch.float16   True
blocks.0.cross_attn.to_q.bias                                           torch.Size([768])               torch.float16   True
blocks.0.cross_attn.to_kv.weight                                        torch.Size([1536, 768])         torch.float16   True
blocks.0.cross_attn.to_kv.bias                                          torch.Size([1536])              torch.float16   True
blocks.0.cross_attn.to_out.weight                                       torch.Size([768, 768])          torch.float16   True
blocks.0.cross_attn.to_out.bias                                         torch.Size([768])               torch.float16   True
blocks.0.mlp.mlp.0.weight                                               torch.Size([3072, 768])         torch.float16   True
blocks.0.mlp.mlp.0.bias                                                 torch.Size([3072])              torch.float16   True
blocks.0.mlp.mlp.2.weight                                               torch.Size([768, 3072])         torch.float16   True
blocks.0.mlp.mlp.2.bias                                                 torch.Size([768])               torch.float16   True
blocks.0.adaLN_modulation.1.weight                                      torch.Size([4608, 768])         torch.float16   True
blocks.0.adaLN_modulation.1.bias                                        torch.Size([4608])              torch.float16   True
blocks.1.norm2.weight                                                   torch.Size([768])               torch.float32   True
blocks.1.norm2.bias                                                     torch.Size([768])               torch.float32   True
blocks.1.self_attn.to_qkv.weight                                        torch.Size([2304, 768])         torch.float16   True
blocks.1.self_attn.to_qkv.bias                                          torch.Size([2304])              torch.float16   True
blocks.1.self_attn.q_rms_norm.gamma                                     torch.Size([12, 64])            torch.float32   True
blocks.1.self_attn.k_rms_norm.gamma                                     torch.Size([12, 64])            torch.float32   True
blocks.1.self_attn.to_out.weight                                        torch.Size([768, 768])          torch.float16   True
blocks.1.self_attn.to_out.bias                                          torch.Size([768])               torch.float16   True
blocks.1.cross_attn.to_q.weight                                         torch.Size([768, 768])          torch.float16   True
blocks.1.cross_attn.to_q.bias                                           torch.Size([768])               torch.float16   True
blocks.1.cross_attn.to_kv.weight                                        torch.Size([1536, 768])         torch.float16   True
blocks.1.cross_attn.to_kv.bias                                          torch.Size([1536])              torch.float16   True
blocks.1.cross_attn.to_out.weight                                       torch.Size([768, 768])          torch.float16   True
blocks.1.cross_attn.to_out.bias                                         torch.Size([768])               torch.float16   True
blocks.1.mlp.mlp.0.weight                                               torch.Size([3072, 768])         torch.float16   True
blocks.1.mlp.mlp.0.bias                                                 torch.Size([3072])              torch.float16   True
blocks.1.mlp.mlp.2.weight                                               torch.Size([768, 3072])         torch.float16   True
blocks.1.mlp.mlp.2.bias                                                 torch.Size([768])               torch.float16   True
blocks.1.adaLN_modulation.1.weight                                      torch.Size([4608, 768])         torch.float16   True
blocks.1.adaLN_modulation.1.bias                                        torch.Size([4608])              torch.float16   True
blocks.2.norm2.weight                                                   torch.Size([768])               torch.float32   True
blocks.2.norm2.bias                                                     torch.Size([768])               torch.float32   True
blocks.2.self_attn.to_qkv.weight                                        torch.Size([2304, 768])         torch.float16   True
blocks.2.self_attn.to_qkv.bias                                          torch.Size([2304])              torch.float16   True
blocks.2.self_attn.q_rms_norm.gamma                                     torch.Size([12, 64])            torch.float32   True
blocks.2.self_attn.k_rms_norm.gamma                                     torch.Size([12, 64])            torch.float32   True
blocks.2.self_attn.to_out.weight                                        torch.Size([768, 768])          torch.float16   True
blocks.2.self_attn.to_out.bias                                          torch.Size([768])               torch.float16   True
blocks.2.cross_attn.to_q.weight                                         torch.Size([768, 768])          torch.float16   True
blocks.2.cross_attn.to_q.bias                                           torch.Size([768])               torch.float16   True
blocks.2.cross_attn.to_kv.weight                                        torch.Size([1536, 768])         torch.float16   True
blocks.2.cross_attn.to_kv.bias                                          torch.Size([1536])              torch.float16   True
blocks.2.cross_attn.to_out.weight                                       torch.Size([768, 768])          torch.float16   True
blocks.2.cross_attn.to_out.bias                                         torch.Size([768])               torch.float16   True
blocks.2.mlp.mlp.0.weight                                               torch.Size([3072, 768])         torch.float16   True
blocks.2.mlp.mlp.0.bias                                                 torch.Size([3072])              torch.float16   True
blocks.2.mlp.mlp.2.weight                                               torch.Size([768, 3072])         torch.float16   True
blocks.2.mlp.mlp.2.bias                                                 torch.Size([768])               torch.float16   True
blocks.2.adaLN_modulation.1.weight                                      torch.Size([4608, 768])         torch.float16   True
blocks.2.adaLN_modulation.1.bias                                        torch.Size([4608])              torch.float16   True
blocks.3.norm2.weight                                                   torch.Size([768])               torch.float32   True
blocks.3.norm2.bias                                                     torch.Size([768])               torch.float32   True
blocks.3.self_attn.to_qkv.weight                                        torch.Size([2304, 768])         torch.float16   True
blocks.3.self_attn.to_qkv.bias                                          torch.Size([2304])              torch.float16   True
blocks.3.self_attn.q_rms_norm.gamma                                     torch.Size([12, 64])            torch.float32   True
blocks.3.self_attn.k_rms_norm.gamma                                     torch.Size([12, 64])            torch.float32   True
blocks.3.self_attn.to_out.weight                                        torch.Size([768, 768])          torch.float16   True
blocks.3.self_attn.to_out.bias                                          torch.Size([768])               torch.float16   True
blocks.3.cross_attn.to_q.weight                                         torch.Size([768, 768])          torch.float16   True
blocks.3.cross_attn.to_q.bias                                           torch.Size([768])               torch.float16   True
blocks.3.cross_attn.to_kv.weight                                        torch.Size([1536, 768])         torch.float16   True
blocks.3.cross_attn.to_kv.bias                                          torch.Size([1536])              torch.float16   True
blocks.3.cross_attn.to_out.weight                                       torch.Size([768, 768])          torch.float16   True
blocks.3.cross_attn.to_out.bias                                         torch.Size([768])               torch.float16   True
blocks.3.mlp.mlp.0.weight                                               torch.Size([3072, 768])         torch.float16   True
blocks.3.mlp.mlp.0.bias                                                 torch.Size([3072])              torch.float16   True
blocks.3.mlp.mlp.2.weight                                               torch.Size([768, 3072])         torch.float16   True
blocks.3.mlp.mlp.2.bias                                                 torch.Size([768])               torch.float16   True
blocks.3.adaLN_modulation.1.weight                                      torch.Size([4608, 768])         torch.float16   True
blocks.3.adaLN_modulation.1.bias                                        torch.Size([4608])              torch.float16   True
blocks.4.norm2.weight                                                   torch.Size([768])               torch.float32   True
blocks.4.norm2.bias                                                     torch.Size([768])               torch.float32   True
blocks.4.self_attn.to_qkv.weight                                        torch.Size([2304, 768])         torch.float16   True
blocks.4.self_attn.to_qkv.bias                                          torch.Size([2304])              torch.float16   True
blocks.4.self_attn.q_rms_norm.gamma                                     torch.Size([12, 64])            torch.float32   True
blocks.4.self_attn.k_rms_norm.gamma                                     torch.Size([12, 64])            torch.float32   True
blocks.4.self_attn.to_out.weight                                        torch.Size([768, 768])          torch.float16   True
blocks.4.self_attn.to_out.bias                                          torch.Size([768])               torch.float16   True
blocks.4.cross_attn.to_q.weight                                         torch.Size([768, 768])          torch.float16   True
blocks.4.cross_attn.to_q.bias                                           torch.Size([768])               torch.float16   True
blocks.4.cross_attn.to_kv.weight                                        torch.Size([1536, 768])         torch.float16   True
blocks.4.cross_attn.to_kv.bias                                          torch.Size([1536])              torch.float16   True
blocks.4.cross_attn.to_out.weight                                       torch.Size([768, 768])          torch.float16   True
blocks.4.cross_attn.to_out.bias                                         torch.Size([768])               torch.float16   True
blocks.4.mlp.mlp.0.weight                                               torch.Size([3072, 768])         torch.float16   True
blocks.4.mlp.mlp.0.bias                                                 torch.Size([3072])              torch.float16   True
blocks.4.mlp.mlp.2.weight                                               torch.Size([768, 3072])         torch.float16   True
blocks.4.mlp.mlp.2.bias                                                 torch.Size([768])               torch.float16   True
blocks.4.adaLN_modulation.1.weight                                      torch.Size([4608, 768])         torch.float16   True
blocks.4.adaLN_modulation.1.bias                                        torch.Size([4608])              torch.float16   True
blocks.5.norm2.weight                                                   torch.Size([768])               torch.float32   True
blocks.5.norm2.bias                                                     torch.Size([768])               torch.float32   True
blocks.5.self_attn.to_qkv.weight                                        torch.Size([2304, 768])         torch.float16   True
blocks.5.self_attn.to_qkv.bias                                          torch.Size([2304])              torch.float16   True
blocks.5.self_attn.q_rms_norm.gamma                                     torch.Size([12, 64])            torch.float32   True
blocks.5.self_attn.k_rms_norm.gamma                                     torch.Size([12, 64])            torch.float32   True
blocks.5.self_attn.to_out.weight                                        torch.Size([768, 768])          torch.float16   True
blocks.5.self_attn.to_out.bias                                          torch.Size([768])               torch.float16   True
blocks.5.cross_attn.to_q.weight                                         torch.Size([768, 768])          torch.float16   True
blocks.5.cross_attn.to_q.bias                                           torch.Size([768])               torch.float16   True
blocks.5.cross_attn.to_kv.weight                                        torch.Size([1536, 768])         torch.float16   True
blocks.5.cross_attn.to_kv.bias                                          torch.Size([1536])              torch.float16   True
blocks.5.cross_attn.to_out.weight                                       torch.Size([768, 768])          torch.float16   True
blocks.5.cross_attn.to_out.bias                                         torch.Size([768])               torch.float16   True
blocks.5.mlp.mlp.0.weight                                               torch.Size([3072, 768])         torch.float16   True
blocks.5.mlp.mlp.0.bias                                                 torch.Size([3072])              torch.float16   True
blocks.5.mlp.mlp.2.weight                                               torch.Size([768, 3072])         torch.float16   True
blocks.5.mlp.mlp.2.bias                                                 torch.Size([768])               torch.float16   True
blocks.5.adaLN_modulation.1.weight                                      torch.Size([4608, 768])         torch.float16   True
blocks.5.adaLN_modulation.1.bias                                        torch.Size([4608])              torch.float16   True
blocks.6.norm2.weight                                                   torch.Size([768])               torch.float32   True
blocks.6.norm2.bias                                                     torch.Size([768])               torch.float32   True
blocks.6.self_attn.to_qkv.weight                                        torch.Size([2304, 768])         torch.float16   True
blocks.6.self_attn.to_qkv.bias                                          torch.Size([2304])              torch.float16   True
blocks.6.self_attn.q_rms_norm.gamma                                     torch.Size([12, 64])            torch.float32   True
blocks.6.self_attn.k_rms_norm.gamma                                     torch.Size([12, 64])            torch.float32   True
blocks.6.self_attn.to_out.weight                                        torch.Size([768, 768])          torch.float16   True
blocks.6.self_attn.to_out.bias                                          torch.Size([768])               torch.float16   True
blocks.6.cross_attn.to_q.weight                                         torch.Size([768, 768])          torch.float16   True
blocks.6.cross_attn.to_q.bias                                           torch.Size([768])               torch.float16   True
blocks.6.cross_attn.to_kv.weight                                        torch.Size([1536, 768])         torch.float16   True
blocks.6.cross_attn.to_kv.bias                                          torch.Size([1536])              torch.float16   True
blocks.6.cross_attn.to_out.weight                                       torch.Size([768, 768])          torch.float16   True
blocks.6.cross_attn.to_out.bias                                         torch.Size([768])               torch.float16   True
blocks.6.mlp.mlp.0.weight                                               torch.Size([3072, 768])         torch.float16   True
blocks.6.mlp.mlp.0.bias                                                 torch.Size([3072])              torch.float16   True
blocks.6.mlp.mlp.2.weight                                               torch.Size([768, 3072])         torch.float16   True
blocks.6.mlp.mlp.2.bias                                                 torch.Size([768])               torch.float16   True
blocks.6.adaLN_modulation.1.weight                                      torch.Size([4608, 768])         torch.float16   True
blocks.6.adaLN_modulation.1.bias                                        torch.Size([4608])              torch.float16   True
blocks.7.norm2.weight                                                   torch.Size([768])               torch.float32   True
blocks.7.norm2.bias                                                     torch.Size([768])               torch.float32   True
blocks.7.self_attn.to_qkv.weight                                        torch.Size([2304, 768])         torch.float16   True
blocks.7.self_attn.to_qkv.bias                                          torch.Size([2304])              torch.float16   True
blocks.7.self_attn.q_rms_norm.gamma                                     torch.Size([12, 64])            torch.float32   True
blocks.7.self_attn.k_rms_norm.gamma                                     torch.Size([12, 64])            torch.float32   True
blocks.7.self_attn.to_out.weight                                        torch.Size([768, 768])          torch.float16   True
blocks.7.self_attn.to_out.bias                                          torch.Size([768])               torch.float16   True
blocks.7.cross_attn.to_q.weight                                         torch.Size([768, 768])          torch.float16   True
blocks.7.cross_attn.to_q.bias                                           torch.Size([768])               torch.float16   True
blocks.7.cross_attn.to_kv.weight                                        torch.Size([1536, 768])         torch.float16   True
blocks.7.cross_attn.to_kv.bias                                          torch.Size([1536])              torch.float16   True
blocks.7.cross_attn.to_out.weight                                       torch.Size([768, 768])          torch.float16   True
blocks.7.cross_attn.to_out.bias                                         torch.Size([768])               torch.float16   True
blocks.7.mlp.mlp.0.weight                                               torch.Size([3072, 768])         torch.float16   True
blocks.7.mlp.mlp.0.bias                                                 torch.Size([3072])              torch.float16   True
blocks.7.mlp.mlp.2.weight                                               torch.Size([768, 3072])         torch.float16   True
blocks.7.mlp.mlp.2.bias                                                 torch.Size([768])               torch.float16   True
blocks.7.adaLN_modulation.1.weight                                      torch.Size([4608, 768])         torch.float16   True
blocks.7.adaLN_modulation.1.bias                                        torch.Size([4608])              torch.float16   True
blocks.8.norm2.weight                                                   torch.Size([768])               torch.float32   True
blocks.8.norm2.bias                                                     torch.Size([768])               torch.float32   True
blocks.8.self_attn.to_qkv.weight                                        torch.Size([2304, 768])         torch.float16   True
blocks.8.self_attn.to_qkv.bias                                          torch.Size([2304])              torch.float16   True
blocks.8.self_attn.q_rms_norm.gamma                                     torch.Size([12, 64])            torch.float32   True
blocks.8.self_attn.k_rms_norm.gamma                                     torch.Size([12, 64])            torch.float32   True
blocks.8.self_attn.to_out.weight                                        torch.Size([768, 768])          torch.float16   True
blocks.8.self_attn.to_out.bias                                          torch.Size([768])               torch.float16   True
blocks.8.cross_attn.to_q.weight                                         torch.Size([768, 768])          torch.float16   True
blocks.8.cross_attn.to_q.bias                                           torch.Size([768])               torch.float16   True
blocks.8.cross_attn.to_kv.weight                                        torch.Size([1536, 768])         torch.float16   True
blocks.8.cross_attn.to_kv.bias                                          torch.Size([1536])              torch.float16   True
blocks.8.cross_attn.to_out.weight                                       torch.Size([768, 768])          torch.float16   True
blocks.8.cross_attn.to_out.bias                                         torch.Size([768])               torch.float16   True
blocks.8.mlp.mlp.0.weight                                               torch.Size([3072, 768])         torch.float16   True
blocks.8.mlp.mlp.0.bias                                                 torch.Size([3072])              torch.float16   True
blocks.8.mlp.mlp.2.weight                                               torch.Size([768, 3072])         torch.float16   True
blocks.8.mlp.mlp.2.bias                                                 torch.Size([768])               torch.float16   True
blocks.8.adaLN_modulation.1.weight                                      torch.Size([4608, 768])         torch.float16   True
blocks.8.adaLN_modulation.1.bias                                        torch.Size([4608])              torch.float16   True
blocks.9.norm2.weight                                                   torch.Size([768])               torch.float32   True
blocks.9.norm2.bias                                                     torch.Size([768])               torch.float32   True
blocks.9.self_attn.to_qkv.weight                                        torch.Size([2304, 768])         torch.float16   True
blocks.9.self_attn.to_qkv.bias                                          torch.Size([2304])              torch.float16   True
blocks.9.self_attn.q_rms_norm.gamma                                     torch.Size([12, 64])            torch.float32   True
blocks.9.self_attn.k_rms_norm.gamma                                     torch.Size([12, 64])            torch.float32   True
blocks.9.self_attn.to_out.weight                                        torch.Size([768, 768])          torch.float16   True
blocks.9.self_attn.to_out.bias                                          torch.Size([768])               torch.float16   True
blocks.9.cross_attn.to_q.weight                                         torch.Size([768, 768])          torch.float16   True
blocks.9.cross_attn.to_q.bias                                           torch.Size([768])               torch.float16   True
blocks.9.cross_attn.to_kv.weight                                        torch.Size([1536, 768])         torch.float16   True
blocks.9.cross_attn.to_kv.bias                                          torch.Size([1536])              torch.float16   True
blocks.9.cross_attn.to_out.weight                                       torch.Size([768, 768])          torch.float16   True
blocks.9.cross_attn.to_out.bias                                         torch.Size([768])               torch.float16   True
blocks.9.mlp.mlp.0.weight                                               torch.Size([3072, 768])         torch.float16   True
blocks.9.mlp.mlp.0.bias                                                 torch.Size([3072])              torch.float16   True
blocks.9.mlp.mlp.2.weight                                               torch.Size([768, 3072])         torch.float16   True
blocks.9.mlp.mlp.2.bias                                                 torch.Size([768])               torch.float16   True
blocks.9.adaLN_modulation.1.weight                                      torch.Size([4608, 768])         torch.float16   True
blocks.9.adaLN_modulation.1.bias                                        torch.Size([4608])              torch.float16   True
blocks.10.norm2.weight                                                  torch.Size([768])               torch.float32   True
blocks.10.norm2.bias                                                    torch.Size([768])               torch.float32   True
blocks.10.self_attn.to_qkv.weight                                       torch.Size([2304, 768])         torch.float16   True
blocks.10.self_attn.to_qkv.bias                                         torch.Size([2304])              torch.float16   True
blocks.10.self_attn.q_rms_norm.gamma                                    torch.Size([12, 64])            torch.float32   True
blocks.10.self_attn.k_rms_norm.gamma                                    torch.Size([12, 64])            torch.float32   True
blocks.10.self_attn.to_out.weight                                       torch.Size([768, 768])          torch.float16   True
blocks.10.self_attn.to_out.bias                                         torch.Size([768])               torch.float16   True
blocks.10.cross_attn.to_q.weight                                        torch.Size([768, 768])          torch.float16   True
blocks.10.cross_attn.to_q.bias                                          torch.Size([768])               torch.float16   True
blocks.10.cross_attn.to_kv.weight                                       torch.Size([1536, 768])         torch.float16   True
blocks.10.cross_attn.to_kv.bias                                         torch.Size([1536])              torch.float16   True
blocks.10.cross_attn.to_out.weight                                      torch.Size([768, 768])          torch.float16   True
blocks.10.cross_attn.to_out.bias                                        torch.Size([768])               torch.float16   True
blocks.10.mlp.mlp.0.weight                                              torch.Size([3072, 768])         torch.float16   True
blocks.10.mlp.mlp.0.bias                                                torch.Size([3072])              torch.float16   True
blocks.10.mlp.mlp.2.weight                                              torch.Size([768, 3072])         torch.float16   True
blocks.10.mlp.mlp.2.bias                                                torch.Size([768])               torch.float16   True
blocks.10.adaLN_modulation.1.weight                                     torch.Size([4608, 768])         torch.float16   True
blocks.10.adaLN_modulation.1.bias                                       torch.Size([4608])              torch.float16   True
blocks.11.norm2.weight                                                  torch.Size([768])               torch.float32   True
blocks.11.norm2.bias                                                    torch.Size([768])               torch.float32   True
blocks.11.self_attn.to_qkv.weight                                       torch.Size([2304, 768])         torch.float16   True
blocks.11.self_attn.to_qkv.bias                                         torch.Size([2304])              torch.float16   True
blocks.11.self_attn.q_rms_norm.gamma                                    torch.Size([12, 64])            torch.float32   True
blocks.11.self_attn.k_rms_norm.gamma                                    torch.Size([12, 64])            torch.float32   True
blocks.11.self_attn.to_out.weight                                       torch.Size([768, 768])          torch.float16   True
blocks.11.self_attn.to_out.bias                                         torch.Size([768])               torch.float16   True
blocks.11.cross_attn.to_q.weight                                        torch.Size([768, 768])          torch.float16   True
blocks.11.cross_attn.to_q.bias                                          torch.Size([768])               torch.float16   True
blocks.11.cross_attn.to_kv.weight                                       torch.Size([1536, 768])         torch.float16   True
blocks.11.cross_attn.to_kv.bias                                         torch.Size([1536])              torch.float16   True
blocks.11.cross_attn.to_out.weight                                      torch.Size([768, 768])          torch.float16   True
blocks.11.cross_attn.to_out.bias                                        torch.Size([768])               torch.float16   True
blocks.11.mlp.mlp.0.weight                                              torch.Size([3072, 768])         torch.float16   True
blocks.11.mlp.mlp.0.bias                                                torch.Size([3072])              torch.float16   True
blocks.11.mlp.mlp.2.weight                                              torch.Size([768, 3072])         torch.float16   True
blocks.11.mlp.mlp.2.bias                                                torch.Size([768])               torch.float16   True
blocks.11.adaLN_modulation.1.weight                                     torch.Size([4608, 768])         torch.float16   True
blocks.11.adaLN_modulation.1.bias                                       torch.Size([4608])              torch.float16   True
out_blocks.0.norm1.weight                                               torch.Size([1536])              torch.float32   True
out_blocks.0.norm1.bias                                                 torch.Size([1536])              torch.float32   True
out_blocks.0.conv1.conv.weight                                          torch.Size([128, 3, 3, 3, 1536])torch.float16   True
out_blocks.0.conv1.conv.bias                                            torch.Size([128])               torch.float16   True
out_blocks.0.conv2.conv.weight                                          torch.Size([128, 3, 3, 3, 128]) torch.float16   True
out_blocks.0.conv2.conv.bias                                            torch.Size([128])               torch.float16   True
out_blocks.0.emb_layers.1.weight                                        torch.Size([256, 768])          torch.float16   True
out_blocks.0.emb_layers.1.bias                                          torch.Size([256])               torch.float16   True
out_blocks.0.skip_connection.weight                                     torch.Size([128, 1536])         torch.float16   True
out_blocks.0.skip_connection.bias                                       torch.Size([128])               torch.float16   True
out_blocks.1.norm1.weight                                               torch.Size([256])               torch.float32   True
out_blocks.1.norm1.bias                                                 torch.Size([256])               torch.float32   True
out_blocks.1.conv1.conv.weight                                          torch.Size([128, 3, 3, 3, 256]) torch.float16   True
out_blocks.1.conv1.conv.bias                                            torch.Size([128])               torch.float16   True
out_blocks.1.conv2.conv.weight                                          torch.Size([128, 3, 3, 3, 128]) torch.float16   True
out_blocks.1.conv2.conv.bias                                            torch.Size([128])               torch.float16   True
out_blocks.1.emb_layers.1.weight                                        torch.Size([256, 768])          torch.float16   True
out_blocks.1.emb_layers.1.bias                                          torch.Size([256])               torch.float16   True
out_blocks.1.skip_connection.weight                                     torch.Size([128, 256])          torch.float16   True
out_blocks.1.skip_connection.bias                                       torch.Size([128])               torch.float16   True
out_layer.weight                                                        torch.Size([8, 128])            torch.float32   True
out_layer.bias                                                          torch.Size([8])                 torch.float32   True

Number of parameters: 185364616
Number of trainable parameters: 185364616


Finetuning from:
  - denoiser: /fs-computility/ai-shen/wujianyu/TRELLIS/microsoft/TRELLIS-text-base/ckpts/slat_flow_txt_dit_B_64l8p2_fp16.safetensors
Load from safetensors
-----------------------------------
Done.

Performing DDP check...
Checking if parameters are consistent across processes...
Done.

Performing DDP check...
Checking if parameters are consistent across processes...
Done.


Trainer initialized.
TextConditionedSparseFlowMatchingCFGTrainer
  - Models:
    - denoiser: ElasticSLatFlowModel
  - Dataset: TextConditionedSLat
    - Total instances: 200
    - Sources:
      - T8:
        - Total: 200
        - With latent: 200
        - Aesthetic score >= 0: 200
        - Num voxels <= 32768: 200
        - With captions: 200
  - Dataloader:
    - Sampler: BalancedResumableSampler
    - Num workers: 32
  - Number of steps: 1000000
  - Number of GPUs: 4
  - Batch size: 64
  - Batch size per GPU: 16
  - Batch split: 4
  - Optimizer: AdamW
  - Learning rate: 0.0001
  - Elastic memory: LinearMemoryController(target_ratio=0.75, available_memory=79.34686279296875)
  - Gradient clip: AdaptiveGradClipper(max_norm=1.0, clip_percentile=95)
  - EMA rate: [0.9999]
  - FP16 mode: inflat_all

Starting training...

Sampling 64 images... Done.
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  _TORCH_CUSTOM_FWD = amp.custom_fwd(cast_inputs=torch.float16)
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:97: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:163: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:243: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:332: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:369: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:389: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:412: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  _TORCH_CUSTOM_FWD = amp.custom_fwd(cast_inputs=torch.float16)
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:97: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:163: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:243: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:332: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:369: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:389: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:412: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  _TORCH_CUSTOM_FWD = amp.custom_fwd(cast_inputs=torch.float16)
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:97: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:163: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:243: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:332: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:369: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:389: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:412: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  _TORCH_CUSTOM_FWD = amp.custom_fwd(cast_inputs=torch.float16)
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:97: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:163: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:243: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:332: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:369: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:389: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:412: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  _TORCH_CUSTOM_FWD = amp.custom_fwd(cast_inputs=torch.float16)
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:97: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:163: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:243: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:332: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:369: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:389: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:412: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  _TORCH_CUSTOM_FWD = amp.custom_fwd(cast_inputs=torch.float16)
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:97: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:163: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:243: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:332: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:369: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:389: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:412: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  _TORCH_CUSTOM_FWD = amp.custom_fwd(cast_inputs=torch.float16)
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:97: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:163: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:243: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:332: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:369: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:389: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:412: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  _TORCH_CUSTOM_FWD = amp.custom_fwd(cast_inputs=torch.float16)
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:97: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:163: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:243: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:332: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:369: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:389: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:412: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  _TORCH_CUSTOM_FWD = amp.custom_fwd(cast_inputs=torch.float16)
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:97: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:163: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:243: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:332: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:369: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:389: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:412: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  _TORCH_CUSTOM_FWD = amp.custom_fwd(cast_inputs=torch.float16)
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:97: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:163: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:243: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:332: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:369: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:389: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:412: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  _TORCH_CUSTOM_FWD = amp.custom_fwd(cast_inputs=torch.float16)
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:97: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:163: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:243: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:332: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:369: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:389: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:412: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  _TORCH_CUSTOM_FWD = amp.custom_fwd(cast_inputs=torch.float16)
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:97: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:163: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:243: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:332: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:369: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:389: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
/root/miniconda3/envs/trellis/lib/python3.10/site-packages/spconv/pytorch/functional.py:412: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  def backward(ctx, grad_output):
Step: 1000/1000000 (0.10%) | Elapsed: 0.30 h           | Speed: 3371.57 steps/h    | ETA: 296.30 h            
Step: 2000/1000000 (0.20%) | Elapsed: 0.49 h           | Speed: 5300.18 steps/h    | ETA: 188.30 h            

Sampling 64 images... Done.

Saving checkpoint at step 2000... Done.
Step: 3000/1000000 (0.30%) | Elapsed: 0.67 h           | Speed: 5309.38 steps/h    | ETA: 187.78 h            
Step: 4000/1000000 (0.40%) | Elapsed: 0.86 h           | Speed: 5317.05 steps/h    | ETA: 187.32 h            

Sampling 64 images... Done.

Saving checkpoint at step 4000... Done.
Step: 5000/1000000 (0.50%) | Elapsed: 1.05 h           | Speed: 5309.39 steps/h    | ETA: 187.40 h            
Step: 6000/1000000 (0.60%) | Elapsed: 1.24 h           | Speed: 5303.50 steps/h    | ETA: 187.42 h            

Sampling 64 images... Done.

Saving checkpoint at step 6000... Done.
Step: 7000/1000000 (0.70%) | Elapsed: 1.42 h           | Speed: 5397.70 steps/h    | ETA: 183.97 h            
Step: 8000/1000000 (0.80%) | Elapsed: 1.61 h           | Speed: 5357.86 steps/h    | ETA: 185.15 h            

Sampling 64 images... Done.

Saving checkpoint at step 8000... Done.
Step: 9000/1000000 (0.90%) | Elapsed: 1.80 h           | Speed: 5394.30 steps/h    | ETA: 183.71 h            
Step: 10000/1000000 (1.00%) | Elapsed: 1.98 h           | Speed: 5394.18 steps/h    | ETA: 183.53 h            

Performing DDP check...
Checking if parameters are consistent across processes...
Done.

Sampling 64 images... Done.

Saving checkpoint at step 10000... Done.
Step: 11000/1000000 (1.10%) | Elapsed: 2.17 h           | Speed: 5389.53 steps/h    | ETA: 183.50 h            
Step: 12000/1000000 (1.20%) | Elapsed: 2.35 h           | Speed: 5413.52 steps/h    | ETA: 182.51 h            

Sampling 64 images... Done.

Saving checkpoint at step 12000... Done.
Step: 13000/1000000 (1.30%) | Elapsed: 2.54 h           | Speed: 5406.07 steps/h    | ETA: 182.57 h            
Step: 14000/1000000 (1.40%) | Elapsed: 2.72 h           | Speed: 5400.95 steps/h    | ETA: 182.56 h            

Sampling 64 images... Done.

Saving checkpoint at step 14000... Done.
Step: 15000/1000000 (1.50%) | Elapsed: 2.91 h           | Speed: 5377.73 steps/h    | ETA: 183.16 h            
Step: 16000/1000000 (1.60%) | Elapsed: 3.09 h           | Speed: 5397.97 steps/h    | ETA: 182.29 h            

Sampling 64 images... Done.

Saving checkpoint at step 16000... Done.
Step: 17000/1000000 (1.70%) | Elapsed: 3.28 h           | Speed: 5365.27 steps/h    | ETA: 183.22 h            
Step: 18000/1000000 (1.80%) | Elapsed: 3.47 h           | Speed: 5374.82 steps/h    | ETA: 182.70 h            

Sampling 64 images... Done.

Saving checkpoint at step 18000... Done.
Step: 19000/1000000 (1.90%) | Elapsed: 3.65 h           | Speed: 5372.72 steps/h    | ETA: 182.59 h            
Step: 20000/1000000 (2.00%) | Elapsed: 3.84 h           | Speed: 5353.92 steps/h    | ETA: 183.04 h            

Performing DDP check...
Checking if parameters are consistent across processes...
Done.

Sampling 64 images... Done.

Saving checkpoint at step 20000... Done.
Step: 21000/1000000 (2.10%) | Elapsed: 4.02 h           | Speed: 5354.52 steps/h    | ETA: 182.84 h            
Step: 22000/1000000 (2.20%) | Elapsed: 4.21 h           | Speed: 5403.13 steps/h    | ETA: 181.01 h            

Sampling 64 images... Done.

Saving checkpoint at step 22000... Done.
Step: 23000/1000000 (2.30%) | Elapsed: 4.39 h           | Speed: 5408.79 steps/h    | ETA: 180.63 h            
Step: 24000/1000000 (2.40%) | Elapsed: 4.58 h           | Speed: 5372.13 steps/h    | ETA: 181.68 h            

Sampling 64 images... Done.

Saving checkpoint at step 24000... Done.
Step: 25000/1000000 (2.50%) | Elapsed: 4.77 h           | Speed: 5355.21 steps/h    | ETA: 182.07 h            
Step: 26000/1000000 (2.60%) | Elapsed: 4.95 h           | Speed: 5392.49 steps/h    | ETA: 180.62 h            

Sampling 64 images... Done.

Saving checkpoint at step 26000... Done.
Step: 27000/1000000 (2.70%) | Elapsed: 5.14 h           | Speed: 5430.61 steps/h    | ETA: 179.17 h            
Step: 28000/1000000 (2.80%) | Elapsed: 5.32 h           | Speed: 5360.25 steps/h    | ETA: 181.33 h            

Sampling 64 images... Done.

Saving checkpoint at step 28000... Done.
Step: 29000/1000000 (2.90%) | Elapsed: 5.51 h           | Speed: 5341.18 steps/h    | ETA: 181.79 h            
Step: 30000/1000000 (3.00%) | Elapsed: 5.70 h           | Speed: 5343.40 steps/h    | ETA: 181.53 h            

Performing DDP check...
Checking if parameters are consistent across processes...
Done.

Sampling 64 images... Done.

Saving checkpoint at step 30000... Done.
Step: 31000/1000000 (3.10%) | Elapsed: 5.89 h           | Speed: 5349.74 steps/h    | ETA: 181.13 h            
Step: 32000/1000000 (3.20%) | Elapsed: 6.07 h           | Speed: 5364.11 steps/h    | ETA: 180.46 h            

Sampling 64 images... Done.

Saving checkpoint at step 32000... Done.
Step: 33000/1000000 (3.30%) | Elapsed: 6.26 h           | Speed: 5343.89 steps/h    | ETA: 180.95 h            
Step: 34000/1000000 (3.40%) | Elapsed: 6.44 h           | Speed: 5394.28 steps/h    | ETA: 179.08 h            

Sampling 64 images... Done.

Saving checkpoint at step 34000... Done.
Step: 35000/1000000 (3.50%) | Elapsed: 6.63 h           | Speed: 5420.32 steps/h    | ETA: 178.03 h            
Step: 36000/1000000 (3.60%) | Elapsed: 6.82 h           | Speed: 5356.04 steps/h    | ETA: 179.98 h            

Sampling 64 images... Done.

Saving checkpoint at step 36000... Done.
Step: 37000/1000000 (3.70%) | Elapsed: 7.00 h           | Speed: 5338.77 steps/h    | ETA: 180.38 h            
Step: 38000/1000000 (3.80%) | Elapsed: 7.19 h           | Speed: 5344.55 steps/h    | ETA: 180.00 h            

Sampling 64 images... Done.

Saving checkpoint at step 38000... Done.
Step: 39000/1000000 (3.90%) | Elapsed: 7.38 h           | Speed: 5343.67 steps/h    | ETA: 179.84 h            
Step: 40000/1000000 (4.00%) | Elapsed: 7.56 h           | Speed: 5407.40 steps/h    | ETA: 177.53 h            

Performing DDP check...
Checking if parameters are consistent across processes...
Done.

Sampling 64 images... Done.

Saving checkpoint at step 40000... Done.
Step: 41000/1000000 (4.10%) | Elapsed: 7.75 h           | Speed: 5432.77 steps/h    | ETA: 176.52 h            
Step: 42000/1000000 (4.20%) | Elapsed: 7.93 h           | Speed: 5389.48 steps/h    | ETA: 177.75 h            

Sampling 64 images... Done.

Saving checkpoint at step 42000... Done.
Step: 43000/1000000 (4.30%) | Elapsed: 8.14 h           | Speed: 4887.33 steps/h    | ETA: 195.81 h            
Step: 44000/1000000 (4.40%) | Elapsed: 8.41 h           | Speed: 3718.16 steps/h    | ETA: 257.12 h            

Sampling 64 images... Done.

Saving checkpoint at step 44000... Done.
Step: 45000/1000000 (4.50%) | Elapsed: 8.67 h           | Speed: 3723.77 steps/h    | ETA: 256.46 h            
Step: 46000/1000000 (4.60%) | Elapsed: 8.94 h           | Speed: 3708.61 steps/h    | ETA: 257.24 h            

Sampling 64 images... Done.

Saving checkpoint at step 46000... Done.
Step: 47000/1000000 (4.70%) | Elapsed: 9.21 h           | Speed: 3705.57 steps/h    | ETA: 257.18 h            
Step: 48000/1000000 (4.80%) | Elapsed: 9.48 h           | Speed: 3699.17 steps/h    | ETA: 257.35 h            

Sampling 64 images... Done.

Saving checkpoint at step 48000... Done.
Step: 49000/1000000 (4.90%) | Elapsed: 9.75 h           | Speed: 3708.64 steps/h    | ETA: 256.43 h            
Step: 50000/1000000 (5.00%) | Elapsed: 10.02 h          | Speed: 3697.01 steps/h    | ETA: 256.96 h            

Performing DDP check...
Checking if parameters are consistent across processes...
Done.

Sampling 64 images... Done.

Saving checkpoint at step 50000... Done.
Step: 51000/1000000 (5.10%) | Elapsed: 10.29 h          | Speed: 3690.64 steps/h    | ETA: 257.14 h            
Step: 52000/1000000 (5.20%) | Elapsed: 10.56 h          | Speed: 3700.15 steps/h    | ETA: 256.21 h            

Sampling 64 images... Done.

Saving checkpoint at step 52000... Done.
Step: 53000/1000000 (5.30%) | Elapsed: 10.84 h          | Speed: 3690.50 steps/h    | ETA: 256.60 h            
Step: 54000/1000000 (5.40%) | Elapsed: 11.11 h          | Speed: 3695.51 steps/h    | ETA: 255.99 h            

Sampling 64 images... Done.

Saving checkpoint at step 54000... Done.
Step: 55000/1000000 (5.50%) | Elapsed: 11.37 h          | Speed: 3748.55 steps/h    | ETA: 252.10 h            
Step: 56000/1000000 (5.60%) | Elapsed: 11.59 h          | Speed: 4699.92 steps/h    | ETA: 200.85 h            

Sampling 64 images... Done.

Saving checkpoint at step 56000... Done.
Step: 57000/1000000 (5.70%) | Elapsed: 11.77 h          | Speed: 5327.92 steps/h    | ETA: 176.99 h            
Step: 58000/1000000 (5.80%) | Elapsed: 11.96 h          | Speed: 5350.53 steps/h    | ETA: 176.06 h            

Sampling 64 images... Done.

Saving checkpoint at step 58000... Done.
Step: 59000/1000000 (5.90%) | Elapsed: 12.15 h          | Speed: 5346.98 steps/h    | ETA: 175.99 h            
Step: 60000/1000000 (6.00%) | Elapsed: 12.34 h          | Speed: 5300.46 steps/h    | ETA: 177.34 h            

Performing DDP check...
Checking if parameters are consistent across processes...
Done.

Sampling 64 images... Done.

Saving checkpoint at step 60000... Done.
Step: 61000/1000000 (6.10%) | Elapsed: 12.53 h          | Speed: 5265.21 steps/h    | ETA: 178.34 h            
Step: 62000/1000000 (6.20%) | Elapsed: 12.71 h          | Speed: 5295.86 steps/h    | ETA: 177.12 h            

Sampling 64 images... Done.

Saving checkpoint at step 62000... Done.
Step: 63000/1000000 (6.30%) | Elapsed: 12.90 h          | Speed: 5307.50 steps/h    | ETA: 176.54 h            
Step: 64000/1000000 (6.40%) | Elapsed: 13.09 h          | Speed: 5311.16 steps/h    | ETA: 176.23 h            

Sampling 64 images... Done.

Saving checkpoint at step 64000... Done.
Step: 65000/1000000 (6.50%) | Elapsed: 13.28 h          | Speed: 5289.34 steps/h    | ETA: 176.77 h            
Step: 66000/1000000 (6.60%) | Elapsed: 13.47 h          | Speed: 5268.42 steps/h    | ETA: 177.28 h            

Sampling 64 images... Done.

Saving checkpoint at step 66000... Done.
Step: 67000/1000000 (6.70%) | Elapsed: 13.66 h          | Speed: 5311.82 steps/h    | ETA: 175.65 h            
Step: 68000/1000000 (6.80%) | Elapsed: 13.85 h          | Speed: 5343.17 steps/h    | ETA: 174.43 h            

Sampling 64 images... Done.

Saving checkpoint at step 68000... Done.
Step: 69000/1000000 (6.90%) | Elapsed: 14.03 h          | Speed: 5310.40 steps/h    | ETA: 175.32 h            
Step: 70000/1000000 (7.00%) | Elapsed: 14.22 h          | Speed: 5332.59 steps/h    | ETA: 174.40 h            

Performing DDP check...
Checking if parameters are consistent across processes...
Done.

Sampling 64 images... Done.

Saving checkpoint at step 70000... Done.
Step: 71000/1000000 (7.10%) | Elapsed: 14.41 h          | Speed: 5323.03 steps/h    | ETA: 174.52 h            
Step: 72000/1000000 (7.20%) | Elapsed: 14.60 h          | Speed: 5389.01 steps/h    | ETA: 172.20 h            

Sampling 64 images... Done.

Saving checkpoint at step 72000... Done.
Step: 73000/1000000 (7.30%) | Elapsed: 14.79 h          | Speed: 5242.28 steps/h    | ETA: 176.83 h            
Step: 74000/1000000 (7.40%) | Elapsed: 14.98 h          | Speed: 5180.74 steps/h    | ETA: 178.74 h            

Sampling 64 images... Done.

Saving checkpoint at step 74000... Done.
Step: 75000/1000000 (7.50%) | Elapsed: 15.17 h          | Speed: 5301.17 steps/h    | ETA: 174.49 h            
Step: 76000/1000000 (7.60%) | Elapsed: 15.36 h          | Speed: 5242.16 steps/h    | ETA: 176.26 h            

Sampling 64 images... Done.

Saving checkpoint at step 76000... Done.
Step: 77000/1000000 (7.70%) | Elapsed: 15.55 h          | Speed: 5245.19 steps/h    | ETA: 175.97 h            
Step: 78000/1000000 (7.80%) | Elapsed: 15.74 h          | Speed: 5233.65 steps/h    | ETA: 176.17 h            

Sampling 64 images... Done.

Saving checkpoint at step 78000... Done.
Step: 79000/1000000 (7.90%) | Elapsed: 15.93 h          | Speed: 5265.56 steps/h    | ETA: 174.91 h            
Step: 80000/1000000 (8.00%) | Elapsed: 16.12 h          | Speed: 5221.49 steps/h    | ETA: 176.20 h            

Performing DDP check...
Checking if parameters are consistent across processes...
Done.

Sampling 64 images... Done.

Saving checkpoint at step 80000... Done.
Step: 81000/1000000 (8.10%) | Elapsed: 16.31 h          | Speed: 5294.71 steps/h    | ETA: 173.57 h            
Step: 82000/1000000 (8.20%) | Elapsed: 16.50 h          | Speed: 5261.06 steps/h    | ETA: 174.49 h            

Sampling 64 images... Done.

Saving checkpoint at step 82000... Done.
Step: 83000/1000000 (8.30%) | Elapsed: 16.69 h          | Speed: 5285.07 steps/h    | ETA: 173.51 h            
Step: 84000/1000000 (8.40%) | Elapsed: 16.88 h          | Speed: 5232.42 steps/h    | ETA: 175.06 h            

Sampling 64 images... Done.

Saving checkpoint at step 84000... Done.
Step: 85000/1000000 (8.50%) | Elapsed: 17.07 h          | Speed: 5215.49 steps/h    | ETA: 175.44 h            
Step: 86000/1000000 (8.60%) | Elapsed: 17.26 h          | Speed: 5214.06 steps/h    | ETA: 175.30 h            

Sampling 64 images... Done.

Saving checkpoint at step 86000... Done.
Step: 87000/1000000 (8.70%) | Elapsed: 17.45 h          | Speed: 5337.62 steps/h    | ETA: 171.05 h            
Step: 88000/1000000 (8.80%) | Elapsed: 17.64 h          | Speed: 5394.42 steps/h    | ETA: 169.06 h            

Sampling 64 images... Done.

Saving checkpoint at step 88000... Done.
Step: 89000/1000000 (8.90%) | Elapsed: 17.82 h          | Speed: 5344.97 steps/h    | ETA: 170.44 h            
Step: 90000/1000000 (9.00%) | Elapsed: 18.01 h          | Speed: 5328.54 steps/h    | ETA: 170.78 h            

Performing DDP check...
Checking if parameters are consistent across processes...
Done.

Sampling 64 images... Done.

Saving checkpoint at step 90000... Done.
Step: 91000/1000000 (9.10%) | Elapsed: 18.20 h          | Speed: 5358.82 steps/h    | ETA: 169.63 h            
Step: 92000/1000000 (9.20%) | Elapsed: 18.39 h          | Speed: 5356.78 steps/h    | ETA: 169.50 h            

Sampling 64 images... Done.

Saving checkpoint at step 92000... Done.
Step: 93000/1000000 (9.30%) | Elapsed: 18.57 h          | Speed: 5305.46 steps/h    | ETA: 170.96 h            
Step: 94000/1000000 (9.40%) | Elapsed: 18.76 h          | Speed: 5313.30 steps/h    | ETA: 170.52 h            

Sampling 64 images... Done.

Saving checkpoint at step 94000... Done.
Step: 95000/1000000 (9.50%) | Elapsed: 18.95 h          | Speed: 5370.48 steps/h    | ETA: 168.51 h            
Step: 96000/1000000 (9.60%) | Elapsed: 19.13 h          | Speed: 5368.73 steps/h    | ETA: 168.38 h            

Sampling 64 images... Done.

Saving checkpoint at step 96000... Done.
Step: 97000/1000000 (9.70%) | Elapsed: 19.32 h          | Speed: 5377.49 steps/h    | ETA: 167.92 h            
Step: 98000/1000000 (9.80%) | Elapsed: 19.51 h          | Speed: 5345.07 steps/h    | ETA: 168.75 h            

Sampling 64 images... Done.

Saving checkpoint at step 98000... Done.
Step: 99000/1000000 (9.90%) | Elapsed: 19.69 h          | Speed: 5396.17 steps/h    | ETA: 166.97 h            
Step: 100000/1000000 (10.00%) | Elapsed: 19.88 h          | Speed: 5406.20 steps/h    | ETA: 166.48 h            

Performing DDP check...
Checking if parameters are consistent across processes...
Done.

Sampling 64 images... Done.

Saving checkpoint at step 100000... Done.
Step: 101000/1000000 (10.10%) | Elapsed: 20.06 h          | Speed: 5358.79 steps/h    | ETA: 167.76 h            
Step: 102000/1000000 (10.20%) | Elapsed: 20.25 h          | Speed: 5370.09 steps/h    | ETA: 167.22 h            

Sampling 64 images... Done.

Saving checkpoint at step 102000... Done.
Step: 103000/1000000 (10.30%) | Elapsed: 20.44 h          | Speed: 5348.78 steps/h    | ETA: 167.70 h            
Step: 104000/1000000 (10.40%) | Elapsed: 20.62 h          | Speed: 5341.71 steps/h    | ETA: 167.74 h            

Sampling 64 images... Done.

Saving checkpoint at step 104000... Done.
Step: 105000/1000000 (10.50%) | Elapsed: 20.81 h          | Speed: 5366.52 steps/h    | ETA: 166.77 h            
Step: 106000/1000000 (10.60%) | Elapsed: 21.00 h          | Speed: 5377.85 steps/h    | ETA: 166.24 h            

Sampling 64 images... Done.

Saving checkpoint at step 106000... Done.
Step: 107000/1000000 (10.70%) | Elapsed: 21.18 h          | Speed: 5370.36 steps/h    | ETA: 166.28 h            
Step: 108000/1000000 (10.80%) | Elapsed: 21.37 h          | Speed: 5413.91 steps/h    | ETA: 164.76 h            

Sampling 64 images... Done.

Saving checkpoint at step 108000... Done.
Step: 109000/1000000 (10.90%) | Elapsed: 21.55 h          | Speed: 5343.69 steps/h    | ETA: 166.74 h            
Step: 110000/1000000 (11.00%) | Elapsed: 21.74 h          | Speed: 5381.14 steps/h    | ETA: 165.39 h            

Performing DDP check...
Checking if parameters are consistent across processes...
Done.

Sampling 64 images... Done.

Saving checkpoint at step 110000... Done.
Step: 111000/1000000 (11.10%) | Elapsed: 21.93 h          | Speed: 5340.90 steps/h    | ETA: 166.45 h            
Step: 112000/1000000 (11.20%) | Elapsed: 22.11 h          | Speed: 5382.31 steps/h    | ETA: 164.98 h            

Sampling 64 images... Done.

Saving checkpoint at step 112000... Done.
Step: 113000/1000000 (11.30%) | Elapsed: 22.30 h          | Speed: 5351.10 steps/h    | ETA: 165.76 h            
Step: 114000/1000000 (11.40%) | Elapsed: 22.49 h          | Speed: 5391.64 steps/h    | ETA: 164.33 h            

Sampling 64 images... Done.

Saving checkpoint at step 114000... Done.
Step: 115000/1000000 (11.50%) | Elapsed: 22.67 h          | Speed: 5332.57 steps/h    | ETA: 165.96 h            
Step: 116000/1000000 (11.60%) | Elapsed: 22.86 h          | Speed: 5354.84 steps/h    | ETA: 165.08 h            

Sampling 64 images... Done.

Saving checkpoint at step 116000... Done.
Step: 117000/1000000 (11.70%) | Elapsed: 23.05 h          | Speed: 5411.27 steps/h    | ETA: 163.18 h            
Step: 118000/1000000 (11.80%) | Elapsed: 23.23 h          | Speed: 5415.48 steps/h    | ETA: 162.87 h            

Sampling 64 images... Done.

Saving checkpoint at step 118000... Done.
Step: 119000/1000000 (11.90%) | Elapsed: 23.41 h          | Speed: 5404.81 steps/h    | ETA: 163.00 h            
Step: 120000/1000000 (12.00%) | Elapsed: 23.61 h          | Speed: 5031.55 steps/h    | ETA: 174.90 h            

Performing DDP check...
Checking if parameters are consistent across processes...
Done.

Sampling 64 images... Done.

Saving checkpoint at step 120000... Done.
Step: 121000/1000000 (12.10%) | Elapsed: 23.91 h          | Speed: 3416.58 steps/h    | ETA: 257.27 h            
Step: 122000/1000000 (12.20%) | Elapsed: 24.20 h          | Speed: 3454.12 steps/h    | ETA: 254.19 h            

Sampling 64 images... Done.

Saving checkpoint at step 122000... Done.
Step: 123000/1000000 (12.30%) | Elapsed: 24.49 h          | Speed: 3429.71 steps/h    | ETA: 255.71 h            
Step: 124000/1000000 (12.40%) | Elapsed: 24.77 h          | Speed: 3477.75 steps/h    | ETA: 251.89 h            

Sampling 64 images... Done.

Saving checkpoint at step 124000... Done.
Step: 125000/1000000 (12.50%) | Elapsed: 25.06 h          | Speed: 3534.03 steps/h    | ETA: 247.59 h            
Step: 126000/1000000 (12.60%) | Elapsed: 25.27 h          | Speed: 4751.47 steps/h    | ETA: 183.94 h            

Sampling 64 images... Done.

Saving checkpoint at step 126000... Done.
Step: 127000/1000000 (12.70%) | Elapsed: 25.47 h          | Speed: 4966.41 steps/h    | ETA: 175.78 h            
Step: 128000/1000000 (12.80%) | Elapsed: 25.67 h          | Speed: 5095.38 steps/h    | ETA: 171.14 h            

Sampling 64 images... Done.

Saving checkpoint at step 128000... Done.
Step: 129000/1000000 (12.90%) | Elapsed: 25.85 h          | Speed: 5321.01 steps/h    | ETA: 163.69 h            
Step: 130000/1000000 (13.00%) | Elapsed: 26.04 h          | Speed: 5363.56 steps/h    | ETA: 162.21 h            

Performing DDP check...
Checking if parameters are consistent across processes...
Done.

Sampling 64 images... Done.

Saving checkpoint at step 130000... Done.
Step: 131000/1000000 (13.10%) | Elapsed: 26.23 h          | Speed: 5392.49 steps/h    | ETA: 161.15 h            
Step: 132000/1000000 (13.20%) | Elapsed: 26.41 h          | Speed: 5429.49 steps/h    | ETA: 159.87 h            

Sampling 64 images... Done.

Saving checkpoint at step 132000... Done.
Step: 133000/1000000 (13.30%) | Elapsed: 26.60 h          | Speed: 5339.58 steps/h    | ETA: 162.37 h            
Step: 134000/1000000 (13.40%) | Elapsed: 26.78 h          | Speed: 5386.43 steps/h    | ETA: 160.77 h            

Sampling 64 images... Done.

Saving checkpoint at step 134000... Done.
Step: 135000/1000000 (13.50%) | Elapsed: 26.97 h          | Speed: 5376.43 steps/h    | ETA: 160.89 h            
Step: 136000/1000000 (13.60%) | Elapsed: 27.15 h          | Speed: 5395.18 steps/h    | ETA: 160.14 h            

Sampling 64 images... Done.

Saving checkpoint at step 136000... Done.
Step: 137000/1000000 (13.70%) | Elapsed: 27.34 h          | Speed: 5391.51 steps/h    | ETA: 160.07 h            
Step: 138000/1000000 (13.80%) | Elapsed: 27.52 h          | Speed: 5405.44 steps/h    | ETA: 159.47 h            

Sampling 64 images... Done.

Saving checkpoint at step 138000... Done.
Step: 139000/1000000 (13.90%) | Elapsed: 27.71 h          | Speed: 5353.89 steps/h    | ETA: 160.82 h            
Step: 140000/1000000 (14.00%) | Elapsed: 27.90 h          | Speed: 5385.53 steps/h    | ETA: 159.69 h            

Performing DDP check...
Checking if parameters are consistent across processes...
Done.

Sampling 64 images... Done.

Saving checkpoint at step 140000... Done.
Step: 141000/1000000 (14.10%) | Elapsed: 28.08 h          | Speed: 5393.71 steps/h    | ETA: 159.26 h            
Step: 142000/1000000 (14.20%) | Elapsed: 28.27 h          | Speed: 5364.64 steps/h    | ETA: 159.94 h            

Sampling 64 images... Done.

Saving checkpoint at step 142000... Done.
Step: 143000/1000000 (14.30%) | Elapsed: 28.46 h          | Speed: 5353.67 steps/h    | ETA: 160.08 h            
Step: 144000/1000000 (14.40%) | Elapsed: 28.64 h          | Speed: 5346.65 steps/h    | ETA: 160.10 h            

Sampling 64 images... Done.

Saving checkpoint at step 144000... Done.
Step: 145000/1000000 (14.50%) | Elapsed: 28.83 h          | Speed: 5390.20 steps/h    | ETA: 158.62 h            
Step: 146000/1000000 (14.60%) | Elapsed: 29.01 h          | Speed: 5397.10 steps/h    | ETA: 158.23 h            

Sampling 64 images... Done.

Saving checkpoint at step 146000... Done.
Step: 147000/1000000 (14.70%) | Elapsed: 29.20 h          | Speed: 5389.89 steps/h    | ETA: 158.26 h            
Step: 148000/1000000 (14.80%) | Elapsed: 29.38 h          | Speed: 5381.78 steps/h    | ETA: 158.31 h            

Sampling 64 images... Done.

Saving checkpoint at step 148000... Done.
Step: 149000/1000000 (14.90%) | Elapsed: 29.57 h          | Speed: 5359.08 steps/h    | ETA: 158.80 h            
Step: 150000/1000000 (15.00%) | Elapsed: 29.76 h          | Speed: 5381.68 steps/h    | ETA: 157.94 h            

Performing DDP check...
Checking if parameters are consistent across processes...
Done.

Sampling 64 images... Done.

Saving checkpoint at step 150000... Done.
Step: 151000/1000000 (15.10%) | Elapsed: 29.94 h          | Speed: 5334.42 steps/h    | ETA: 159.15 h            
Step: 152000/1000000 (15.20%) | Elapsed: 30.13 h          | Speed: 5361.32 steps/h    | ETA: 158.17 h            

Sampling 64 images... Done.

Saving checkpoint at step 152000... Done.
Step: 153000/1000000 (15.30%) | Elapsed: 30.32 h          | Speed: 5385.68 steps/h    | ETA: 157.27 h            
Step: 154000/1000000 (15.40%) | Elapsed: 30.50 h          | Speed: 5400.89 steps/h    | ETA: 156.64 h            

Sampling 64 images... Done.

Saving checkpoint at step 154000... Done.
Step: 155000/1000000 (15.50%) | Elapsed: 30.69 h          | Speed: 5385.04 steps/h    | ETA: 156.92 h            
Step: 156000/1000000 (15.60%) | Elapsed: 30.88 h          | Speed: 5255.13 steps/h    | ETA: 160.60 h            

Sampling 64 images... Done.

Saving checkpoint at step 156000... Done.
Step: 157000/1000000 (15.70%) | Elapsed: 31.07 h          | Speed: 5336.10 steps/h    | ETA: 157.98 h            
Step: 158000/1000000 (15.80%) | Elapsed: 31.26 h          | Speed: 5274.08 steps/h    | ETA: 159.65 h            

Sampling 64 images... Done.

Saving checkpoint at step 158000... Done.
Step: 159000/1000000 (15.90%) | Elapsed: 31.44 h          | Speed: 5294.58 steps/h    | ETA: 158.84 h            
Step: 160000/1000000 (16.00%) | Elapsed: 31.63 h          | Speed: 5292.04 steps/h    | ETA: 158.73 h            

Performing DDP check...
Checking if parameters are consistent across processes...
Done.

Sampling 64 images... Done.

Saving checkpoint at step 160000... Done.
Step: 161000/1000000 (16.10%) | Elapsed: 31.82 h          | Speed: 5337.82 steps/h    | ETA: 157.18 h            
Step: 162000/1000000 (16.20%) | Elapsed: 32.01 h          | Speed: 5315.08 steps/h    | ETA: 157.66 h            

Sampling 64 images... Done.

Saving checkpoint at step 162000... Done.
Step: 163000/1000000 (16.30%) | Elapsed: 32.20 h          | Speed: 5267.31 steps/h    | ETA: 158.90 h            
Step: 164000/1000000 (16.40%) | Elapsed: 32.39 h          | Speed: 5283.23 steps/h    | ETA: 158.24 h            

Sampling 64 images... Done.

Saving checkpoint at step 164000... Done.
Step: 165000/1000000 (16.50%) | Elapsed: 32.58 h          | Speed: 5257.17 steps/h    | ETA: 158.83 h            
Step: 166000/1000000 (16.60%) | Elapsed: 32.77 h          | Speed: 5300.69 steps/h    | ETA: 157.34 h            

Sampling 64 images... Done.

Saving checkpoint at step 166000... Done.
Step: 167000/1000000 (16.70%) | Elapsed: 32.96 h          | Speed: 5300.90 steps/h    | ETA: 157.14 h            
Step: 168000/1000000 (16.80%) | Elapsed: 33.14 h          | Speed: 5319.86 steps/h    | ETA: 156.40 h            

Sampling 64 images... Done.

Saving checkpoint at step 168000... Done.
Step: 169000/1000000 (16.90%) | Elapsed: 33.33 h          | Speed: 5309.37 steps/h    | ETA: 156.52 h            
Step: 170000/1000000 (17.00%) | Elapsed: 33.52 h          | Speed: 5311.13 steps/h    | ETA: 156.28 h            

Performing DDP check...
Checking if parameters are consistent across processes...
Done.

Sampling 64 images... Done.

Saving checkpoint at step 170000... Done.
Step: 171000/1000000 (17.10%) | Elapsed: 33.71 h          | Speed: 5310.05 steps/h    | ETA: 156.12 h            
Step: 172000/1000000 (17.20%) | Elapsed: 33.89 h          | Speed: 5383.69 steps/h    | ETA: 153.80 h            

Sampling 64 images... Done.

Saving checkpoint at step 172000... Done.
Step: 173000/1000000 (17.30%) | Elapsed: 34.08 h          | Speed: 5363.65 steps/h    | ETA: 154.19 h            
Step: 174000/1000000 (17.40%) | Elapsed: 34.27 h          | Speed: 5349.01 steps/h    | ETA: 154.42 h            

Sampling 64 images... Done.

Saving checkpoint at step 174000... Done.
Step: 175000/1000000 (17.50%) | Elapsed: 34.45 h          | Speed: 5373.16 steps/h    | ETA: 153.54 h            
Step: 176000/1000000 (17.60%) | Elapsed: 34.64 h          | Speed: 5383.68 steps/h    | ETA: 153.06 h            

Sampling 64 images... Done.

Saving checkpoint at step 176000... Done.
Step: 177000/1000000 (17.70%) | Elapsed: 34.82 h          | Speed: 5384.60 steps/h    | ETA: 152.84 h            
Step: 178000/1000000 (17.80%) | Elapsed: 35.01 h          | Speed: 5351.87 steps/h    | ETA: 153.59 h            

Sampling 64 images... Done.

Saving checkpoint at step 178000... Done.
Step: 179000/1000000 (17.90%) | Elapsed: 35.20 h          | Speed: 5362.09 steps/h    | ETA: 153.11 h            
Step: 180000/1000000 (18.00%) | Elapsed: 35.38 h          | Speed: 5364.02 steps/h    | ETA: 152.87 h            

Performing DDP check...
Checking if parameters are consistent across processes...
Done.

Sampling 64 images... Done.

Saving checkpoint at step 180000... Done.
Step: 181000/1000000 (18.10%) | Elapsed: 35.57 h          | Speed: 5364.71 steps/h    | ETA: 152.66 h            
Step: 182000/1000000 (18.20%) | Elapsed: 35.76 h          | Speed: 5378.57 steps/h    | ETA: 152.09 h            

Sampling 64 images... Done.

Saving checkpoint at step 182000... Done.
Step: 183000/1000000 (18.30%) | Elapsed: 35.94 h          | Speed: 5349.05 steps/h    | ETA: 152.74 h            
Step: 184000/1000000 (18.40%) | Elapsed: 36.13 h          | Speed: 5375.85 steps/h    | ETA: 151.79 h            

Sampling 64 images... Done.

Saving checkpoint at step 184000... Done.
Step: 185000/1000000 (18.50%) | Elapsed: 36.31 h          | Speed: 5424.10 steps/h    | ETA: 150.26 h            
Step: 186000/1000000 (18.60%) | Elapsed: 36.50 h          | Speed: 5381.48 steps/h    | ETA: 151.26 h            

Sampling 64 images... Done.

Saving checkpoint at step 186000... Done.
Step: 187000/1000000 (18.70%) | Elapsed: 36.69 h          | Speed: 5363.69 steps/h    | ETA: 151.57 h            
Step: 188000/1000000 (18.80%) | Elapsed: 36.87 h          | Speed: 5349.47 steps/h    | ETA: 151.79 h            

Sampling 64 images... Done.

Saving checkpoint at step 188000... Done.
Step: 189000/1000000 (18.90%) | Elapsed: 37.06 h          | Speed: 5394.88 steps/h    | ETA: 150.33 h            
Step: 190000/1000000 (19.00%) | Elapsed: 37.24 h          | Speed: 5428.84 steps/h    | ETA: 149.20 h            

Performing DDP check...
Checking if parameters are consistent across processes...
Done.

Sampling 64 images... Done.

Saving checkpoint at step 190000... Done.
Step: 191000/1000000 (19.10%) | Elapsed: 37.43 h          | Speed: 5392.79 steps/h    | ETA: 150.02 h            
Step: 192000/1000000 (19.20%) | Elapsed: 37.61 h          | Speed: 5409.54 steps/h    | ETA: 149.37 h            

Sampling 64 images... Done.

Saving checkpoint at step 192000... Done.
Step: 193000/1000000 (19.30%) | Elapsed: 37.80 h          | Speed: 5372.44 steps/h    | ETA: 150.21 h            
Step: 194000/1000000 (19.40%) | Elapsed: 37.99 h          | Speed: 5376.11 steps/h    | ETA: 149.92 h            

Sampling 64 images... Done.

Saving checkpoint at step 194000... Done.
Step: 195000/1000000 (19.50%) | Elapsed: 38.17 h          | Speed: 5397.87 steps/h    | ETA: 149.13 h            
Step: 196000/1000000 (19.60%) | Elapsed: 38.36 h          | Speed: 5354.18 steps/h    | ETA: 150.16 h            

Sampling 64 images... Done.

Saving checkpoint at step 196000... Done.
Step: 197000/1000000 (19.70%) | Elapsed: 38.54 h          | Speed: 5388.48 steps/h    | ETA: 149.02 h            
Step: 198000/1000000 (19.80%) | Elapsed: 38.73 h          | Speed: 5416.33 steps/h    | ETA: 148.07 h            

Sampling 64 images... Done.

Saving checkpoint at step 198000... Done.
Step: 199000/1000000 (19.90%) | Elapsed: 38.91 h          | Speed: 5413.98 steps/h    | ETA: 147.95 h            
Step: 200000/1000000 (20.00%) | Elapsed: 39.10 h          | Speed: 5385.18 steps/h    | ETA: 148.56 h            

Performing DDP check...
Checking if parameters are consistent across processes...
Done.

Sampling 64 images... Done.

Saving checkpoint at step 200000... Done.
Step: 201000/1000000 (20.10%) | Elapsed: 39.28 h          | Speed: 5415.24 steps/h    | ETA: 147.55 h            
Step: 202000/1000000 (20.20%) | Elapsed: 39.47 h          | Speed: 5263.43 steps/h    | ETA: 151.61 h            

Sampling 64 images... Done.

Saving checkpoint at step 202000... Done.
Step: 203000/1000000 (20.30%) | Elapsed: 39.66 h          | Speed: 5216.41 steps/h    | ETA: 152.79 h            
Step: 204000/1000000 (20.40%) | Elapsed: 39.86 h          | Speed: 5214.69 steps/h    | ETA: 152.65 h            

Sampling 64 images... Done.

Saving checkpoint at step 204000... Done.
Step: 205000/1000000 (20.50%) | Elapsed: 40.05 h          | Speed: 5214.66 steps/h    | ETA: 152.45 h            
Step: 206000/1000000 (20.60%) | Elapsed: 40.24 h          | Speed: 5210.01 steps/h    | ETA: 152.40 h            

Sampling 64 images... Done.

Saving checkpoint at step 206000... Done.
Step: 207000/1000000 (20.70%) | Elapsed: 40.43 h          | Speed: 5189.74 steps/h    | ETA: 152.80 h            
Step: 208000/1000000 (20.80%) | Elapsed: 40.62 h          | Speed: 5197.62 steps/h    | ETA: 152.38 h            

Sampling 64 images... Done.

Saving checkpoint at step 208000... Done.
Step: 209000/1000000 (20.90%) | Elapsed: 40.82 h          | Speed: 5210.63 steps/h    | ETA: 151.81 h            
Step: 210000/1000000 (21.00%) | Elapsed: 41.01 h          | Speed: 5233.75 steps/h    | ETA: 150.94 h            

Performing DDP check...
Checking if parameters are consistent across processes...
Done.

Sampling 64 images... Done.

Saving checkpoint at step 210000... Done.
Step: 211000/1000000 (21.10%) | Elapsed: 41.20 h          | Speed: 5208.36 steps/h    | ETA: 151.49 h            
Step: 212000/1000000 (21.20%) | Elapsed: 41.39 h          | Speed: 5195.75 steps/h    | ETA: 151.66 h            

Sampling 64 images... Done.

Saving checkpoint at step 212000... Done.
Step: 213000/1000000 (21.30%) | Elapsed: 41.58 h          | Speed: 5198.64 steps/h    | ETA: 151.39 h            
Step: 214000/1000000 (21.40%) | Elapsed: 41.78 h          | Speed: 5234.19 steps/h    | ETA: 150.17 h            

Sampling 64 images... Done.

Saving checkpoint at step 214000... Done.
Step: 215000/1000000 (21.50%) | Elapsed: 41.97 h          | Speed: 5214.03 steps/h    | ETA: 150.56 h            
